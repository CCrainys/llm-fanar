# Callibration

A good benchmark is one that clearly shows which models are better and which are worse. We test our benchmark tasks by using a series of progressively more advanced models to see if the benchmarks effectively differentiate between them, and at which number of shots they performed best at.

To run the code:
* Select an independant variable and a series of models that correspond to the settings of that variable
* Select clusters 
* Edit the list of tasks in the base_callibration.yaml to reflect the ones you want to see
* Run the launcher script
* When everything is done, run the analyze_output notebook which collates the results from wandb


