
icl_tasks:
-
  label: truthful_qa_mc2
  dataset_uri: hf://eitanturok/truthful_qa
  hf_loading_vars:
    name: mc2
    split: training
  num_fewshot: [0]
  question_prelimiter: "Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.\n\nQ: "
  continuation_delimiter: "\nA:"
  icl_task_type: multiple_choice
  metric_names:
    - InContextLearningMultipleChoiceMultipleAnswersProb
-
  label: arc_challenge
  dataset_uri: eval/local_data/world_knowledge/arc_challenge.jsonl
  num_fewshot: [25]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
-
  label: hellaswag
  dataset_uri: eval/local_data/language_understanding/hellaswag.jsonl
  num_fewshot: [10]
  icl_task_type: multiple_choice
-
  label: mmlu
  dataset_uri: eval/local_data/world_knowledge/mmlu.jsonl
  num_fewshot: [5]
  icl_task_type: multiple_choice
  continuation_delimiter: "\nAnswer: "
  has_categories: true
-
  label: winogrande
  dataset_uri: eval/local_data/language_understanding/winogrande.jsonl
  num_fewshot: [5]
  icl_task_type: schema
-
  label: gsm8k
  dataset_uri: eval/local_data/symbolic_problem_solving/gsm8k_prepended_8shot.jsonl
  num_fewshot: [0]
  icl_task_type: question_answering
  cot_delimiter: "The answer is "
  continuation_delimiter: "\n\nA:"
  question_prelimiter: ""
  do_normalization: false
  early_stopping_criteria:
  - "\n\n"
  - "Question:"
